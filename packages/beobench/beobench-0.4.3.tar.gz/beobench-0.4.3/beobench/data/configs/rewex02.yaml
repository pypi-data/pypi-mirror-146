# REWEX Experiment 02
# Run with the command
# beobench run -c beobench/data/configs/rewex01_test02.yaml -d .[extended] --use-gpu --docker-shm-size 28gb

# Some of the descriptions of RLlib config values are taken from
# https://docs.ray.io/en/latest/rllib/rllib-training.html

# agent config
agent:
  origin: rllib # either path to agent script or name of agent library (rllib)
  config: # given to ray.tune.run() as arguments (since rllib set before)
    run_or_experiment: PPO
    stop:
      timesteps_total: 35040
    config:
      lr: 0.0005
      model:
        fcnet_activation: relu
        fcnet_hiddens: [256,256,256,256]
        post_fcnet_activation: tanh
      batch_mode: complete_episodes
      gamma: 0.999
      # Number of steps after which the episode is forced to terminate. Defaults
      # to `env.spec.max_episode_steps` (if present) for Gym envs.
      horizon: 96
      # Calculate rewards but don't reset the environment when the horizon is
      # hit. This allows value estimation and RNN state to span across logical
      # episodes denoted by horizon. This only has an effect if horizon != inf.
      soft_horizon: True
      # Number of timesteps collected for each SGD round. This defines the size
      # of each SGD epoch.
      train_batch_size: 94 # single day of 15min steps
      # Total SGD batch size across all devices for SGD. This defines the
      # minibatch size within each epoch.
      sgd_minibatch_size: 24
      metrics_smoothing_episodes: 1
      framework: torch
      log_level: "WARNING"
      num_workers: 1 # this is required for energym to work (can fail silently otherwise)
      num_gpus: 1
# environment config
env:
  name: MixedUseFanFCU-v0
  gym: energym
  config:
    days: 365
    energym_environment: MixedUseFanFCU-v0
    gym_kwargs:
      max_episode_length: 35040
      normalize: true
      step_period: 15
    weather: GRC_A_Athens