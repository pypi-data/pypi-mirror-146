# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02zb_modeling-seq2seq-language-modeling.ipynb (unless otherwise specified).

__all__ = []

# Cell
import ast

import torch
from transformers import *
from fastai.text.all import *

from ...utils import *
from ...data.core import *
from ...data.seq2seq.core import *
from ...data.seq2seq.language_modeling import *
from ..core import *

# Cell
@typedispatch
def show_results(x:HF_MLMInput, y, samples, outs, learner, ctxs=None, max_n=6, trunc_at=None, **kwargs):
    # grab our tokenizer and ignore token to decode
    hf_before_batch_tfm = get_blurr_tfm(learner.dls.before_batch)
    hf_config = hf_before_batch_tfm.hf_config
    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer
    ignore_token_id = hf_before_batch_tfm.ignore_token_id

    # grab our mask token id and do-not-mask token ids
    mask_token_id = hf_tokenizer.mask_token_id
    dnm_tok_ids = hf_before_batch_tfm.lm_strategy.dnm_tok_ids

    res = L()
    for s, t in zip(samples, outs):
        # exclue dnm tokens from input
        inps = [ hf_tokenizer.decode(tok_id)
                if (tok_id == mask_token_id or s[1][idx] == ignore_token_id)
                else f'[{hf_tokenizer.decode(tok_id)}]'
                for idx, tok_id in enumerate(s[0]) if (tok_id not in dnm_tok_ids) ]

        # replaced masked tokens with "[{actual_token}]"
        trgs = [ hf_tokenizer.decode(s[0][idx])
                if (tok_id == ignore_token_id)
                else f'[{hf_tokenizer.decode(tok_id)}]'
                for idx, tok_id in enumerate(s[1]) if (s[0][idx] not in dnm_tok_ids) ]

        # same as above except we replace the [MASK] with the PREDICTED token
        preds = [ hf_tokenizer.decode(s[0][idx])
                 if (tok_id == ignore_token_id)
                 else f'[{hf_tokenizer.decode(t[0][idx])}]'
                 for idx, tok_id in enumerate(s[1]) if (s[0][idx] not in dnm_tok_ids) ]

        res.append((' '.join(inps[:trunc_at]).strip(),
                    ' '.join(trgs[:trunc_at]).strip(),
                    ' '.join(preds[:trunc_at]).strip()))

    display_df(pd.DataFrame(res, columns=['text', 'target', 'prediction'])[:max_n])
    return ctxs