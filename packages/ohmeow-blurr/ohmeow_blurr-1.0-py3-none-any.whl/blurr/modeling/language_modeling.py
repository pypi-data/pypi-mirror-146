# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_modeling-language-modeling.ipynb (unless otherwise specified).

__all__ = ['LM_MetricsCallback', 'BlearnerForLM']

# Cell
import os, ast, inspect
from typing import Any, Callable, Dict, List, Optional, Union, Type

from fastcore.all import *
from fastai.callback.all import *
from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter
from fastai.data.core import DataLoader, DataLoaders, TfmdDL
from fastai.imports import *
from fastai.learner import *
from fastai.losses import CrossEntropyLossFlat
from fastai.optimizer import Adam, OptimWrapper, params
from fastai.metrics import perplexity
from fastai.torch_core import *
from fastai.torch_imports import *
from fastprogress.fastprogress import progress_bar,master_bar
from sklearn.metrics import accuracy_score
from transformers import (
    AutoModelForCausalLM, AutoModelForMaskedLM, logging,
    PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel
)


from ..utils import BLURR
from ..data.core import HF_TextBlock, BlurrDataLoader, first_blurr_tfm
from .core import HF_PreCalculatedLoss, Blearner
from ..data.language_modeling import (
    HF_LMBeforeBatchTransform, LMType, LMStrategy, HF_CausalLMInput, CausalLMStrategy, HF_MLMInput, BertMLMStrategy
)

logging.set_verbosity_error()

# Cell
class LM_MetricsCallback(Callback):
    """A fastai friendly metric implemented as a callback so that we can handle use cases where we don't
    want to count tokens marked to be ignored or else not count batches where there are no targs
    """
    def __init__(self, **kwargs):
        self.run_before = Recorder

        self.custom_metrics_dict = { 'lm_accuracy':None }
        self.do_setup = True

    def setup(self):
        # one time setup code here.
        if (not self.do_setup): return

        # add custom text generation specific metrics
        custom_metric_keys = self.custom_metrics_dict.keys()
        custom_metrics = L([ ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys ])
        self.learn.metrics = self.learn.metrics + custom_metrics

        self.do_setup = False

    def before_fit(self):
        self.setup()


    # --- batch begin/after phases ---
    def after_batch(self):
        # do this only for validation set
        if (self.training or self.learn.y is None): return

        preds = self.pred.argmax(dim=-1)
        targs = self.yb[0]               # yb is TensorText tuple, item 0 is the data

        msk = torch.where(targs != -100, 1, 0).bool()
        preds = torch.masked_select(preds, msk).cpu()
        targs = torch.masked_select(targs, msk).cpu()

        if (preds.shape[0] == 0):
            return

        self.results += [ (res[0], res[1]) for res in zip(preds, targs) ]


    # --- validation begin/after phases ---
    def before_validate(self): self.results = []

    def after_validate(self):
        if (len(self.results) < 1): return

        preds, targs = map(list, zip(*self.results))
        self.custom_metrics_dict['lm_accuracy'] = accuracy_score(targs, preds)


    # --- for ValueMetric metrics ---
    def metric_value(self, metric_key): return self.custom_metrics_dict[metric_key]

# Cell
@typedispatch
def show_results(
    # This typedispatched `show_results` will be called for `HF_CausalLMInput` typed inputs
    x:HF_CausalLMInput,
    # Your targets
    y,
    # Your raw inputs/targets
    samples,
    # The model's predictions
    outs,
    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into
    # something understandable
    learner,
    # Your `show_results` context
    ctxs=None,
    # The maximum number of items to show
    max_n=6,
     # Any truncation your want applied to your decoded inputs
    trunc_at=None,
    # Any other keyword arguments you want applied to `show_results`
    **kwargs
):
    # grab our tokenizer and ignore token to decode
    tfm = first_blurr_tfm(learner.dls)

    hf_config = tfm.hf_config
    hf_tokenizer = tfm.hf_tokenizer
    ignore_token_id = tfm.ignore_token_id

    res = L([(
        hf_tokenizer.decode(s[0], skip_special_tokens=True)[:trunc_at],
        hf_tokenizer.decode(s[1][s[1] != ignore_token_id], skip_special_tokens=True)[:trunc_at],
        hf_tokenizer.decode(pred[0], skip_special_tokens=True)[:trunc_at],
    ) for s, pred in zip(samples, outs) ])

    display_df(pd.DataFrame(res, columns=['text', 'target', 'prediction'])[:max_n])
    return ctxs

# Cell
@typedispatch
def show_results(
    # This typedispatched `show_results` will be called for `HF_MLMInput` typed inputs
    x:HF_MLMInput,
    # Your targets
    y,
    # Your raw inputs/targets
    samples,
    # The model's predictions
    outs,
    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into
    # something understandable
    learner,
    # Your `show_results` context
    ctxs=None,
    # The maximum number of items to show
    max_n=6,
     # Any truncation your want applied to your decoded inputs
    trunc_at=None,
    # Any other keyword arguments you want applied to `show_results`
    **kwargs
):
    # grab our tokenizer and ignore token to decode
    tfm = first_blurr_tfm(learner.dls)

    hf_config = tfm.hf_config
    hf_tokenizer = tfm.hf_tokenizer
    ignore_token_id = tfm.ignore_token_id

    # grab our mask token id and do-not-mask token ids
    mask_token_id = hf_tokenizer.mask_token_id

    vocab = hf_tokenizer.get_vocab()
    dnm_tok_ids = [ vocab[tok] for tok in list(hf_tokenizer.special_tokens_map.values())
                   if vocab[tok] != mask_token_id ]

    res = L()
    for s, t in zip(samples, outs):
        # exclue dnm tokens from input
        inps = [ hf_tokenizer.decode(tok_id)
                if (tok_id == mask_token_id or s[1][idx] == ignore_token_id)
                else f'[{hf_tokenizer.decode(tok_id)}]'
                for idx, tok_id in enumerate(s[0]) if (tok_id not in dnm_tok_ids) ]

        # replaced masked tokens with "[{actual_token}]"
        trgs = [ hf_tokenizer.decode(s[0][idx])
                if (tok_id == ignore_token_id)
                else f'[{hf_tokenizer.decode(tok_id)}]'
                for idx, tok_id in enumerate(s[1]) if (s[0][idx] not in dnm_tok_ids) ]

        # same as above except we replace the [MASK] with the PREDICTED token
        preds = [ hf_tokenizer.decode(s[0][idx])
                 if (tok_id == ignore_token_id)
                 else f'[{hf_tokenizer.decode(t[0][idx])}]'
                 for idx, tok_id in enumerate(s[1]) if (s[0][idx] not in dnm_tok_ids) ]

        res.append((' '.join(inps[:trunc_at]).strip(),
                    ' '.join(trgs[:trunc_at]).strip(),
                    ' '.join(preds[:trunc_at]).strip()))

    display_df(pd.DataFrame(res, columns=['text', 'target', 'prediction'])[:max_n])
    return ctxs

# Cell
@patch
def blurr_fill_mask(
    self:Learner,
    # Your input_ids or raw text string with a `hf_tokenizer.mask_token`
    inp:Union[List[int], str],
    # The number of predictions you want to return for the [MASK]ed token
    n_preds:int=1,
    # Any other keyword arguments you want applied to text generation
    **kwargs
):
    """For MLM models"""
    # grab the Hugging Face tokenizer from the learner's dls.tfms
    tfm = first_blurr_tfm(self.dls)

    hf_config = tfm.hf_config
    hf_tokenizer = tfm.hf_tokenizer
    tok_kwargs = tfm.tok_kwargs

    # grab the text generation kwargs
    text_gen_kwargs = tfm.text_gen_kwargs if (len(kwargs) == 0) else kwargs

    if (isinstance(inp, str)):
        input_ids = hf_tokenizer.encode(inp, padding=True, truncation=True, return_tensors='pt', **tok_kwargs)
    else:
        # note (10/30/2020): as of pytorch 1.7, this has to be a plain ol tensor (not a subclass of TensorBase)
        input_ids = inp.as_subclass(Tensor)

    input_ids = input_ids.to(self.model.hf_model.device)
    mask_token_index = torch.where(input_ids == hf_tokenizer.mask_token_id)[1]

    outputs = self.model.hf_model(input_ids)
    mask_token_logits = outputs.logits[0, mask_token_index, :]
    preds = torch.topk(mask_token_logits, n_preds, dim=-1).indices[0].tolist()

    outputs = [ inp.replace(hf_tokenizer.mask_token, hf_tokenizer.decode([tok_id]).strip())
               for tok_id in preds ]

    return outputs

# Cell
@delegates(Blearner.__init__)
class BlearnerForLM(Blearner):

    def __init__(
        self,
        dls: DataLoaders,
        hf_model: PreTrainedModel,
        **kwargs
    ):
        kwargs['loss_func'] = HF_PreCalculatedLoss()
        super().__init__(dls, hf_model, **kwargs)

    @classmethod
    def get_model_cls(self, lm_type):
        return AutoModelForCausalLM if (lm_type == LMType.CAUSAL) else AutoModelForMaskedLM

    @classmethod
    def get_metrics_cb(self):
        return LM_MetricsCallback()

    @classmethod
    def _create_learner(
        cls,
        # Your raw dataset
        data,
        # The name or path of the pretrained model you want to fine-tune
        pretrained_model_name_or_path:Optional[Union[str, os.PathLike]],
        # A function to perform any preprocessing required for your Dataset
        preprocess_func:Callable=None,
        # The language modeling strategy (or objective)
        lm_strategy_cls:LMStrategy=CausalLMStrategy,
        # The attribute in your dataset that contains your raw text
        text_attr:str='text',
        # A function that will split your Dataset into a training and validation set
        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters
        dblock_splitter:Callable=RandomSplitter(),
        # Any kwargs to pass to your `DataLoaders`
        dl_kwargs={},
        # Any kwargs to pass to your task specific `Blearner`
        learner_kwargs={}
    ):
        lm_type = lm_strategy_cls.get_lm_type()

        # get our hf objects
        model_cls = cls.get_model_cls(lm_type=lm_type)
        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name_or_path,
                                                                          model_cls=model_cls)

        # if we need to preprocess the raw data before creating our DataLoaders
        if (preprocess_func):
            data = preprocess_func(data, hf_arch, hf_config, hf_tokenizer, hf_model, lm_type, lm_strategy_cls)

        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here
        if (hf_tokenizer.pad_token is None):
            hf_tokenizer.add_special_tokens({'pad_token': '<pad>'})
            hf_config.pad_token_id = hf_tokenizer.get_vocab()['<pad>']
            hf_model.resize_token_embeddings(len(hf_tokenizer))

        # build getters
        if (isinstance(data, pd.DataFrame)):
            get_x = ColReader(text_attr)
        else:
            get_x = ItemGetter(text_attr)

        # build DataBlocks and DataLoaders
        bbtfm = HF_LMBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model, lm_strategy_cls=lm_strategy_cls)

        input_return_type = HF_CausalLMInput if (lm_type == LMType.CAUSAL) else HF_MLMInput
        blocks = (HF_TextBlock(before_batch_tfm=bbtfm, input_return_type=input_return_type), noop)

        dblock = DataBlock(blocks=blocks, get_x=get_x, splitter=dblock_splitter)
        dls = dblock.dataloaders(data, **dl_kwargs.copy())

        # return BLearner instance with default metrics (optional)
        learner_kwargs['metrics'] = learner_kwargs.pop('metrics', [perplexity])
        return cls(dls, hf_model, **learner_kwargs.copy())

    @classmethod
    def from_dataframe(
        cls,
        # Your pandas DataFrame
        df:pd.DataFrame,
        # The name or path of the pretrained model you want to fine-tune
        pretrained_model_name_or_path:Optional[Union[str, os.PathLike]],
        # A function to perform any preprocessing required for your Dataset
        preprocess_func:Callable=None,
        # The language modeling strategy (or objective)
        lm_strategy_cls:LMStrategy=CausalLMStrategy,
        # The attribute in your dataset that contains your raw text
        text_attr:str='text',
        # A function that will split your Dataset into a training and validation set
        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters
        dblock_splitter:Callable=ColSplitter(),
        # Any kwargs to pass to your `DataLoaders`
        dl_kwargs={},
        # Any kwargs to pass to your task specific `Blearner`
        learner_kwargs={}
    ):
        return cls._create_learner(df, pretrained_model_name_or_path, preprocess_func,
                                   lm_strategy_cls, text_attr, dblock_splitter, dl_kwargs, learner_kwargs)


    @classmethod
    def from_csv(
        cls,
        # The path to your csv file
        csv_file:Union[Path, str],
        # The name or path of the pretrained model you want to fine-tune
        pretrained_model_name_or_path:Optional[Union[str, os.PathLike]],
        # A function to perform any preprocessing required for your Dataset
        preprocess_func:Callable=None,
        # The language modeling strategy (or objective)
        lm_strategy_cls:LMStrategy=CausalLMStrategy,
        # The attribute in your dataset that contains your raw text
        text_attr:str='text',
        # A function that will split your Dataset into a training and validation set
        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters
        dblock_splitter:Callable=ColSplitter(),
        # Any kwargs to pass to your `DataLoaders`
        dl_kwargs={},
        # Any kwargs to pass to your task specific `Blearner`
        learner_kwargs={}
    ):
        df = pd.read_csv(csv_file)
        return cls.from_dataframe(df, pretrained_model_name_or_path, preprocess_func,
                                  lm_strategy_cls, text_attr, dblock_splitter,
                                  dl_kwargs, learner_kwargs)

    @classmethod
    def from_dictionaries(
        cls,
        # A list of dictionaries
        ds:List[Dict],
        # The name or path of the pretrained model you want to fine-tune
        pretrained_model_name_or_path:Optional[Union[str, os.PathLike]],
        # A function to perform any preprocessing required for your Dataset
        preprocess_func:Callable=None,
        # The language modeling strategy (or objective)
        lm_strategy_cls:LMStrategy=CausalLMStrategy,
        # The attribute in your dataset that contains your raw text
        text_attr:str='text',
        # A function that will split your Dataset into a training and validation set
        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters
        dblock_splitter:Callable=RandomSplitter(),
        # Any kwargs to pass to your `DataLoaders`
        dl_kwargs={},
        # Any kwargs to pass to your task specific `Blearner`
        learner_kwargs={}
    ):
        return cls._create_learner(ds, pretrained_model_name_or_path, preprocess_func,
                                   lm_strategy_cls, text_attr, dblock_splitter,
                                   dl_kwargs, learner_kwargs)