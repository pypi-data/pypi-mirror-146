General:
  # [...] = your analysis full path (on the host if you are using a container)
  data_dir_signal: "ANALYSES_DIRECTORY/ANALYSIS_NAME/data/TRAINING/for_particle_classification/gamma"
  data_dir_background: "ANALYSES_DIRECTORY/ANALYSIS_NAME/data/TRAINING/for_particle_classification/proton"
  data_sig_file: "TRAINING_classification_tail_gamma_merged.h5"
  data_bkg_file: "TRAINING_classification_tail_proton_merged.h5"
  outdir: "ANALYSES_DIRECTORY/ANALYSIS_NAME/estimators/gamma_hadron_classifier"
  # List of cameras to use (protopipe-MODEL help output for other options)
  cam_id_list: []

# If train_fraction is 1.0, all the TRAINING dataset will be used to train the
# model and benchmarking can only be done from the benchmarking notebook
# TRAINING/benchmarks_DL2_to_classification.ipynb
Split:
  train_fraction: 0.8
  use_same_number_of_sig_and_bkg_for_training: False # Lowest statistics will drive the split

# Optimize the hyper-parameters of the estimator with a grid search
# If True parameters should be provided as lists (for None use [null])
# If False the model used will be the one based on the chosen single-valued hyper-parameters
GridSearchCV:
  use: False # True or False
  # if False the following to variables are irrelevant
  scoring: "roc_auc"
  cv: 2 # cross-validation splitting strategy
  refit: True # Refit the estimator using the best found parameters
  verbose: 1 # 1,2,3,4
  njobs: -1 # int or -1 (all processors)

# Definition of the algorithm/method used and its hyper-parameters
Method:
  name: "sklearn.ensemble.RandomForestClassifier" # DO NOT CHANGE
  target_name: "label" # defined between 0 and 1 (DO NOT CHANGE)
  tuned_parameters:
    # Please, see scikit-learn's API for what each parameter means
    # WARNING: null (not a string) == 'None'
    n_estimators: 100 # integer
    criterion: "gini" # 'gini' or 'entropy'
    max_depth: 20 # null or integer
    min_samples_split: 2 # integer or float
    min_samples_leaf: 1 # integer or float
    min_weight_fraction_leaf: 0.0 # float
    max_features: 3 # 'auto', 'sqrt', 'log2', integer or float
    max_leaf_nodes: null # null or integer
    min_impurity_decrease: 0.0 # float
    bootstrap: False # True or False
    oob_score: False # True or False
    n_jobs: null # null or integer
    random_state: 0 # null or integer or RandomState
    verbose: 0 # integer
    warm_start: False # 'True' or 'False'
    class_weight: null # 'balanced', 'balanced_subsample', null, dict or list of dicts
    ccp_alpha: 0.0 # non-negative float
    max_samples: null # null, integer or float
  use_proba: True # If True output is 'gammaness', else 'score'
  calibrate_output: False # If True calibrate model on test data

# List of the features to use to train the model
# You can:
# - comment/uncomment the ones you see here,
# - add new ones here if they can be evaluated with pandas.DataFrame.eval
# - if not you can propose modifications to protopipe.mva.utils.prepare_data
FeatureList:
  Basic: # single-named, they need to correspond to input data columns
    - "h_max" # Height of shower maximum from stereoscopic reconstruction
    - "impact_dist" # Impact parameter from stereoscopic reconstruction
    - "hillas_width" # Image Width
    - "hillas_length" # Image Length
    - "concentration_pixel" # Percentage of photo-electrons in the brightest pixel
  Derived: # custom evaluations of basic features that will be added to the data
    # column name : expression to evaluate using basic column names
    log10_intensity: log10(hillas_intensity)
    log10_reco_energy: log10(reco_energy) # Averaged-estimated energy of the shower
    log10_reco_energy_tel: log10(reco_energy_tel) # Estimated energy of the shower per telescope

# These cuts select the input data BEFORE training
SigFiducialCuts:
  - "good_image == 1"
  - "is_valid == True"
  - "hillas_intensity > 0"

BkgFiducialCuts:
  - "good_image == 1"
  - "is_valid == True"
  - "hillas_intensity > 0"

Diagnostic:
  # Energy binning (used for reco and true energy)
  energy:
    nbins: 4
    min: 0.0125
    max: 200
